# üéâ SETUP COMPLETE - Next Steps

## ‚úÖ What You Have Now

Your V6 EODHD Hybrid System is fully set up with:

- ‚úÖ PostgreSQL master database (initialized, empty)
- ‚úÖ SQLite working database (initialized, empty)  
- ‚úÖ EODHD API client (tested and working)
- ‚úÖ Database models and configuration
- ‚úÖ 4 data loading scripts ready to run

---

## üì• Download These Scripts

You created the main files, now download these 4 scripts:

1. **[01_load_universe.py](computer:///mnt/user-data/outputs/01_load_universe.py)** - Load all tickers
2. **[02_load_historical_data.py](computer:///mnt/user-data/outputs/02_load_historical_data.py)** - Load price history
3. **[03_setup_weekly_update.py](computer:///mnt/user-data/outputs/03_setup_weekly_update.py)** - Weekly automation
4. **[04_materialize_sqlite.py](computer:///mnt/user-data/outputs/04_materialize_sqlite.py)** - Create working DB

**Put them in:**
```
D:\FolioData\FolioF\PortfolioOptimizationPythonFiles\V6_EODHD_Hybrid\scripts\
```

---

## üöÄ Step-by-Step Data Loading

### Step 1: Load Universe (~5-10 minutes)

This fetches all ticker symbols from all 75 exchanges:

```bash
cd D:\FolioData\FolioF\PortfolioOptimizationPythonFiles\V6_EODHD_Hybrid
python scripts\01_load_universe.py
```

**What it does:**
- Fetches ticker list for each exchange
- Loads ~150,000 tickers into PostgreSQL
- Uses ~75 API calls (1 per exchange)

**Expected output:**
```
üìä Loading symbols for exchange: US
  Retrieved 51,234 symbols from API
  ‚úÖ US: 5,000 new, 0 updated (45.2s)
...
Total tickers loaded: 147,892
```

---

### Step 2: Load Historical Data (~2-8 hours)

**‚ö†Ô∏è IMPORTANT: This takes a LONG time. Plan accordingly!**

This loads 20+ years of monthly price data:

```bash
python scripts\02_load_historical_data.py
```

**What it does:**
- Uses BULK API for maximum efficiency
- Loads monthly data from 1980 to present
- Samples every 3 months initially (can customize)
- Includes splits and dividends
- Uses ~10,000-20,000 API calls total

**Time estimates:**
- US market: 1-2 hours
- All priority exchanges: 3-6 hours
- All 75 exchanges: 8-12 hours (if you want everything)

**Tips:**
- ‚òï Start it before bed or before work
- üìä Monitor progress in logs: `logs\load_historical_data.log`
- ‚è∏Ô∏è You can stop and resume anytime (won't re-download existing data)
- üéØ Script loads priority exchanges first (US, LSE, XETRA, HK, T, TO)

**Expected output:**
```
[1/150] 1980-01-01
  Prices: 15,234
[2/150] 1980-04-01
  Prices: 15,456
...
‚úÖ US complete:
  Prices: 2,345,678
  Splits: 1,234
  Dividends: 5,678
  Time: 87.3 minutes
```

---

### Step 3: Set Up Weekly Updates (~2 minutes)

After initial load, set up automation:

```bash
python scripts\03_setup_weekly_update.py
```

**Options:**
1. **Run update now** - Test the weekly update
2. **Show Windows Task Scheduler setup** - Get command for automation

**To automate (optional):**
1. Choose option 2 to get the command
2. Open Command Prompt as Administrator
3. Run the command to create weekly task
4. Updates will run every Sunday at 2:00 AM

---

### Step 4: Create Working Database (~5 minutes)

Copy your portfolio tickers to fast SQLite database:

```bash
python scripts\04_materialize_sqlite.py
```

**What it does:**
- Copies tickers from PostgreSQL to SQLite
- Includes: Your portfolio + watchlist + benchmarks
- Creates fast, portable database for analysis

**Expected output:**
```
üì¶ Starting materialization of 87 tickers...
[1/87] AAPL.US
  üìà AAPL.US: 240 price records
  üí∞ AAPL.US: 12 corporate actions
...
‚úÖ MATERIALIZATION COMPLETE
Successfully copied: 87
SQLite database: D:\FolioData\FolioF\portfolio_working_v6.db
Database size: 12.3 MB
```

---

## üìù Before Running Step 2 (Historical Load)

### Edit Your Portfolio Tickers (Optional)

Open `config_v6.py` and add your actual holdings:

```python
MY_PORTFOLIO_TICKERS = [
    'AAPL.US',
    'MSFT.US',
    'GOOGL.US',
    # Add your stocks here
]

MY_WATCHLIST_TICKERS = [
    'NVDA.US',
    'TSLA.US',
    # Add research candidates
]
```

This ensures these tickers are prioritized and included in your working database.

---

## ‚è±Ô∏è Time Budget

| Task | Time | API Calls |
|------|------|-----------|
| Load Universe | 5-10 min | 75 |
| Load Historical (US only) | 1-2 hours | ~5,000 |
| Load Historical (all priority) | 3-6 hours | ~15,000 |
| Set Up Weekly Updates | 2 min | 0 |
| Materialize SQLite | 5 min | 0 |
| **TOTAL** | **3-6 hours** | **~15,000** |

---

## üéØ Recommended Approach

### Option A: Quick Start (US Market Only)
```bash
# Day 1 evening:
python scripts\01_load_universe.py        # 10 min

# Before bed:
python scripts\02_load_historical_data.py # Modify to load US only, runs overnight

# Day 2 morning:
python scripts\04_materialize_sqlite.py   # 5 min
# ‚úÖ Ready to use!
```

### Option B: Full Load (All Markets)
```bash
# Day 1:
python scripts\01_load_universe.py        # 10 min

# Weekend:
python scripts\02_load_historical_data.py # Leave running all weekend

# Monday:
python scripts\04_materialize_sqlite.py   # 5 min
# ‚úÖ Complete global database!
```

---

## üîç Monitoring Progress

### Real-time monitoring:
```bash
# Watch main log
tail -f logs\portfolio_system_v6.log

# Watch API usage
tail -f logs\eodhd_api_calls.log
```

### Check API usage:
```bash
python -c "from eodhd_client import EODHDClient; import config_v6; c = EODHDClient(config_v6.EODHD_API_TOKEN); print(c.get_api_usage_today())"
```

---

## ‚ùì FAQs

**Q: Can I stop the historical load and resume later?**
A: Yes! The script checks for existing data and won't re-download it.

**Q: How much API usage will this take?**
A: ~15,000 calls for priority exchanges. You have 100,000/day limit, so plenty of headroom.

**Q: Do I need to load ALL exchanges?**
A: No! Start with US market (edit script to load only US). Add others later if needed.

**Q: How do I add a single ticker later?**
A: `python scripts\04_materialize_sqlite.py --add NVDA.US`

**Q: What if I run out of API calls?**
A: Script will stop. Resume tomorrow - it won't re-download existing data.

---

## üéâ After Setup

Once data is loaded, you can:

1. **Query your data:**
```python
from sqlalchemy import create_engine
import config_v6
from models_v6 import get_session, get_price_data

engine = config_v6.get_sqlite_engine()  # Use SQLite for speed

with get_session(engine) as session:
    prices = get_price_data(session, 'AAPL.US', start_date='2023-01-01')
    print(f"Got {len(prices)} price records")
```

2. **Build your Portfolio Visualizer clone** - Use SQLite working DB

3. **Add more tickers as needed** - Copy from PostgreSQL on-demand

4. **Set up weekly automation** - Keep data fresh automatically

---

## üìû Need Help?

**Common Issues:**

1. **"Database error"** - Check PostgreSQL is running
2. **"API error"** - Check your token in config_v6.py
3. **"No tickers found"** - Run Step 1 first
4. **"Out of API calls"** - Wait until tomorrow, limit resets daily

**Check logs:**
- `logs\portfolio_system_v6.log` - Main log
- `logs\load_historical_data.log` - Historical load details
- `logs\errors.log` - Error details

---

## ‚úÖ You're Ready!

Run these in order:

```bash
python scripts\01_load_universe.py
python scripts\02_load_historical_data.py
python scripts\04_materialize_sqlite.py
```

Then start building your Portfolio Visualizer! üöÄ
